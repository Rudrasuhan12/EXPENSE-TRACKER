{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudrasuhan12/EXPENSE-TRACKER/blob/main/Employee_Salary_Prediction_Notebook_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# =============================================================================\n",
        "# Step 1: Install necessary libraries\n",
        "# We need to install streamlit and pyngrok to run the web app in Colab.\n",
        "# =============================================================================\n",
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 2: Import all required libraries\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from pyngrok import ngrok\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 3: Load the dataset\n",
        "# In Google Colab, you can't use a local file path directly.\n",
        "# This code will prompt you to upload your 'adult.csv' file.\n",
        "# =============================================================================\n",
        "print(\"Please upload your 'adult.csv' file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if a file was uploaded\n",
        "if len(uploaded.keys()) == 0:\n",
        "    print(\"No file uploaded. Please run the cell again and upload the file.\")\n",
        "else:\n",
        "    # Get the filename and load the data\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    print(f\"\\nUploaded file: {file_name}\")\n",
        "    # The file content is in bytes, so we use io.BytesIO to read it into pandas\n",
        "    data = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(\"First 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 4: Data Cleaning and Preprocessing\n",
        "# This section contains all the cleaning and transformation steps you performed.\n",
        "# =============================================================================\n",
        "\n",
        "# --- Handle missing values represented by '?' ---\n",
        "# Replace '?' with a more descriptive 'Others' category.\n",
        "print(\"\\n--- Data Cleaning Started ---\")\n",
        "# Using a dictionary with .replace() on the DataFrame to avoid FutureWarning\n",
        "data.replace({\n",
        "    'workclass': {'?': 'Others'},\n",
        "    'occupation': {'?': 'Others'},\n",
        "    'native-country': {'?': 'Others'}\n",
        "}, inplace=True)\n",
        "print(\"Replaced '?' with 'Others' in categorical columns.\")\n",
        "\n",
        "\n",
        "# --- Remove less frequent categories ---\n",
        "# These categories have very few samples and might not be useful for prediction.\n",
        "data = data[data['workclass'] != 'Without-pay']\n",
        "data = data[data['workclass'] != 'Never-worked']\n",
        "print(\"Removed 'Without-pay' and 'Never-worked' from 'workclass'.\")\n",
        "\n",
        "# --- Remove redundant features ---\n",
        "# The 'education' column is redundant because 'educational-num' represents the same info numerically.\n",
        "if 'education' in data.columns:\n",
        "    data = data.drop(columns=['education'])\n",
        "    print(\"Dropped the redundant 'education' column.\")\n",
        "\n",
        "# --- Handle Outliers ---\n",
        "# Removing extreme outliers based on your analysis to create a more robust model.\n",
        "initial_rows = len(data)\n",
        "data = data[(data['age'] <= 75) & (data['age'] >= 17)]\n",
        "data = data[(data['educational-num'] <= 16) & (data['educational-num'] >= 5)]\n",
        "print(f\"Removed outliers. Rows changed from {initial_rows} to {len(data)}.\")\n",
        "\n",
        "# --- Encode Categorical Features ---\n",
        "# Machine learning models require numerical input. We use LabelEncoder to convert\n",
        "# string categories into numbers. We will save these encoders to use in our app.\n",
        "print(\"\\nEncoding categorical features...\")\n",
        "encoders = {}\n",
        "categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# The target variable 'income' is also categorical, let's encode it separately.\n",
        "if 'income' in categorical_cols:\n",
        "    categorical_cols.remove('income')\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    encoders[col] = le # Save the encoder\n",
        "    print(f\"Encoded '{col}'.\")\n",
        "\n",
        "# --- Save the encoders and column order ---\n",
        "# This is crucial for the Streamlit app to process new data correctly.\n",
        "joblib.dump(encoders, 'encoders.pkl')\n",
        "print(\"\\nSaved all label encoders to 'encoders.pkl'.\")\n",
        "\n",
        "# Save the order of columns the model will be trained on\n",
        "model_columns = data.drop(columns=['income']).columns.tolist()\n",
        "joblib.dump(model_columns, 'model_columns.pkl')\n",
        "print(\"Saved model column order to 'model_columns.pkl'.\")\n",
        "\n",
        "print(\"\\n--- Data Cleaning and Preprocessing Complete! ---\")\n",
        "print(\"Final data preview (first 5 rows):\")\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 5: Model Training and Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n--- Model Training and Evaluation Started ---\")\n",
        "# --- Define Features (X) and Target (y) ---\n",
        "X = data.drop(columns=['income'])\n",
        "y = data['income'] # y remains as string labels ('<=50K', '>50K')\n",
        "\n",
        "# --- Split data into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Data split into training ({len(X_train)} rows) and testing ({len(X_test)} rows) sets.\")\n",
        "\n",
        "# --- Define Models to Compare ---\n",
        "# Using a dictionary to hold all the models we want to test.\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=120, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
        "    \"SVM\": SVC(probability=True, random_state=42), # Enable probability for confidence scores\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=120, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# --- Train, Evaluate, and Find the Best Model ---\n",
        "for name, model in models.items():\n",
        "    # We use a pipeline to scale the data and then train the model.\n",
        "    # This prevents data leakage and is good practice.\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # The pipeline is trained with string labels for y, so it will predict strings.\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[name] = acc\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# --- Identify and Save the Best Model ---\n",
        "best_model_name = max(results, key=results.get)\n",
        "best_model_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', models[best_model_name])\n",
        "])\n",
        "best_model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nüèÜ Best performing model is {best_model_name} with an accuracy of {results[best_model_name]:.4f}\")\n",
        "\n",
        "# Save the entire pipeline (scaler + model)\n",
        "joblib.dump(best_model_pipeline, \"best_model_pipeline.pkl\")\n",
        "print(\"‚úÖ Saved the best model pipeline to 'best_model_pipeline.pkl'.\")\n",
        "print(\"\\n--- Model Training and Evaluation Complete! ---\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 6: Create the Streamlit Web Application\n",
        "# This code writes a file 'app.py' which contains our app's logic.\n",
        "# I've wrapped the string in parentheses to prevent parsing errors.\n",
        "# =============================================================================\n",
        "\n",
        "app_code = (\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- Load Saved Artifacts ---\n",
        "# We load the trained model pipeline, the encoders, and the column order.\n",
        "try:\n",
        "    model_pipeline = joblib.load(\"best_model_pipeline.pkl\")\n",
        "    encoders = joblib.load(\"encoders.pkl\")\n",
        "    model_columns = joblib.load(\"model_columns.pkl\")\n",
        "except FileNotFoundError:\n",
        "    st.error(\"Could not find model files. Please ensure 'best_model_pipeline.pkl', 'encoders.pkl', and 'model_columns.pkl' are in the same directory.\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Employee Salary Prediction\",\n",
        "    page_icon=\"üíº\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# --- App Title and Description ---\n",
        "st.title(\"üíº Employee Salary Prediction\")\n",
        "st.markdown('''\n",
        "This app predicts whether an employee's annual income is **more than $50K** or **less than or equal to $50K**.\n",
        "''')\n",
        "\n",
        "# --- Sidebar for User Inputs ---\n",
        "st.sidebar.header(\"Single Employee Prediction\")\n",
        "st.sidebar.markdown(\"Enter details for one employee below.\")\n",
        "\n",
        "# Create dictionaries of original values for selectboxes.\n",
        "workclass_options = ['Private', 'Self-emp-not-inc', 'Local-gov', 'State-gov', 'Self-emp-inc', 'Federal-gov', 'Others']\n",
        "marital_status_options = ['Married-civ-spouse', 'Never-married', 'Divorced', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse']\n",
        "occupation_options = ['Prof-specialty', 'Craft-repair', 'Exec-managerial', 'Adm-clerical', 'Sales', 'Other-service', 'Machine-op-inspct', 'Transport-moving', 'Handlers-cleaners', 'Farming-fishing', 'Tech-support', 'Protective-serv', 'Priv-house-serv', 'Armed-Forces', 'Others']\n",
        "relationship_options = ['Husband', 'Not-in-family', 'Own-child', 'Unmarried', 'Wife', 'Other-relative']\n",
        "race_options = ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other']\n",
        "gender_options = ['Male', 'Female']\n",
        "native_country_options = ['United-States', 'Mexico', 'Philippines', 'Germany', 'Puerto-Rico', 'Canada', 'El-Salvador', 'India', 'Cuba', 'England', 'Jamaica', 'South', 'China', 'Italy', 'Dominican-Republic', 'Vietnam', 'Guatemala', 'Japan', 'Poland', 'Columbia', 'Taiwan', 'Haiti', 'Iran', 'Portugal', 'Nicaragua', 'Peru', 'France', 'Greece', 'Ecuador', 'Ireland', 'Hong', 'Trinadad&Tobago', 'Cambodia', 'Laos', 'Thailand', 'Yugoslavia', 'Outlying-US(Guam-USVI-etc)', 'Hungary', 'Honduras', 'Scotland', 'Holand-Netherlands', 'Others']\n",
        "\n",
        "\n",
        "# Create input fields in the sidebar\n",
        "age = st.sidebar.slider(\"Age\", 17, 75, 35)\n",
        "workclass = st.sidebar.selectbox(\"Work Class\", options=workclass_options)\n",
        "fnlwgt = st.sidebar.number_input(\"Final Weight (fnlwgt)\", min_value=1, value=189778)\n",
        "educational_num = st.sidebar.slider(\"Education Level (Numeric)\", 5, 16, 10)\n",
        "marital_status = st.sidebar.selectbox(\"Marital Status\", options=marital_status_options)\n",
        "occupation = st.sidebar.selectbox(\"Occupation\", options=occupation_options)\n",
        "relationship = st.sidebar.selectbox(\"Relationship\", options=relationship_options)\n",
        "race = st.sidebar.selectbox(\"Race\", options=race_options)\n",
        "gender = st.sidebar.selectbox(\"Gender\", options=gender_options)\n",
        "capital_gain = st.sidebar.number_input(\"Capital Gain\", min_value=0, value=0)\n",
        "capital_loss = st.sidebar.number_input(\"Capital Loss\", min_value=0, value=0)\n",
        "hours_per_week = st.sidebar.slider(\"Hours per Week\", 1, 99, 40)\n",
        "native_country = st.sidebar.selectbox(\"Native Country\", options=native_country_options)\n",
        "\n",
        "\n",
        "# --- Prediction Logic ---\n",
        "if st.sidebar.button(\"Predict Salary\", use_container_width=True):\n",
        "    # 1. Create a dictionary with the user's input\n",
        "    input_data = {\n",
        "        'age': age, 'workclass': workclass, 'fnlwgt': fnlwgt,\n",
        "        'educational-num': educational_num, 'marital-status': marital_status,\n",
        "        'occupation': occupation, 'relationship': relationship, 'race': race,\n",
        "        'gender': gender, 'capital-gain': capital_gain, 'capital-loss': capital_loss,\n",
        "        'hours-per-week': hours_per_week, 'native-country': native_country\n",
        "    }\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    st.markdown(\"### Your Input (Single Prediction):\")\n",
        "    st.dataframe(input_df)\n",
        "\n",
        "    # 3. Preprocess the input DataFrame\n",
        "    for col, encoder in encoders.items():\n",
        "        input_df[col] = input_df[col].apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1)\n",
        "    input_df = input_df[model_columns]\n",
        "\n",
        "    # 4. Make a prediction\n",
        "    # The model was trained on string labels, so it will predict string labels.\n",
        "    # No need to decode the prediction.\n",
        "    prediction_decoded = model_pipeline.predict(input_df)\n",
        "    prediction_proba = model_pipeline.predict_proba(input_df)\n",
        "\n",
        "    # 5. Display the result\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### Prediction Result\")\n",
        "\n",
        "    if prediction_decoded[0] == '>50K':\n",
        "        st.success(f\"**Predicted Income: >$50K**\")\n",
        "    else:\n",
        "        st.info(f\"**Predicted Income: ‚â§$50K**\")\n",
        "\n",
        "    st.write(\"Prediction Confidence:\")\n",
        "    # The order of probabilities corresponds to model.classes_ which is ['<=50K', '>50K']\n",
        "    st.write(f\"**‚â§$50K:** `{prediction_proba[0][0]:.2%}`\")\n",
        "    st.write(f\"**>$50K:** `{prediction_proba[0][1]:.2%}`\")\n",
        "\n",
        "# --- Batch Prediction from File ---\n",
        "st.markdown(\"---\")\n",
        "st.header(\"üìÇ Batch Prediction from File\")\n",
        "st.markdown(\"Upload a CSV file with the same format as the training data to predict salaries for multiple employees.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    batch_df = pd.read_csv(uploaded_file)\n",
        "    st.markdown(\"### Uploaded Data Preview:\")\n",
        "    st.dataframe(batch_df.head())\n",
        "\n",
        "    predict_df = batch_df.copy()\n",
        "\n",
        "    try:\n",
        "        # Preprocess the uploaded data\n",
        "        for col, encoder in encoders.items():\n",
        "            if col in predict_df.columns:\n",
        "                predict_df[col] = predict_df[col].apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1)\n",
        "\n",
        "        # Ensure columns are in the correct order for the model\n",
        "        predict_df_ordered = predict_df[model_columns]\n",
        "\n",
        "        # Make Predictions\n",
        "        batch_predictions_decoded = model_pipeline.predict(predict_df_ordered)\n",
        "\n",
        "        # Add predictions to the original DataFrame\n",
        "        batch_df['predicted_income'] = batch_predictions_decoded\n",
        "\n",
        "        # Display and Download Results\n",
        "        st.markdown(\"### Prediction Results:\")\n",
        "        st.dataframe(batch_df)\n",
        "\n",
        "        csv = batch_df.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\n",
        "            label=\"Download Predictions as CSV\",\n",
        "            data=csv,\n",
        "            file_name='predicted_salaries.csv',\n",
        "            mime='text/csv',\n",
        "            use_container_width=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during batch prediction: {e}\")\n",
        "        st.warning(\"Please ensure your uploaded CSV has the correct columns (e.g., 'age', 'workclass', etc.) and data types.\")\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ Streamlit app code written to app.py\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# =============================================================================\n",
        "# Step 7: Run the Streamlit App using ngrok\n",
        "# This will create a public URL to access your app.\n",
        "# =============================================================================\n",
        "# --- Robustly terminate any existing ngrok tunnels ---\n",
        "# This is to prevent the ERR_NGROK_108 error (too many sessions).\n",
        "try:\n",
        "    # Get a list of all active tunnels\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    # Disconnect each tunnel\n",
        "    for tunnel in tunnels:\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "    print(\"Disconnected all active ngrok tunnels.\")\n",
        "except Exception as e:\n",
        "    # This might fail if no tunnels are active, which is fine.\n",
        "    print(f\"No active tunnels to disconnect or an error occurred: {e}\")\n",
        "\n",
        "# Kill any existing ngrok process\n",
        "ngrok.kill()\n",
        "print(\"Killed any lingering ngrok processes.\")\n",
        "\n",
        "\n",
        "# --- Set up a new ngrok tunnel ---\n",
        "# We are using the authtoken you provided to authenticate with ngrok.\n",
        "authtoken = \"300Jsjq0oqcC3BEAqCfFVw6LLUw_qXgJEgiAeX4ZUP6HkJ95\"\n",
        "ngrok.set_auth_token(authtoken)\n",
        "\n",
        "# Connect to the port and create a public URL\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"üöÄ Your Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py --server.port 8501 --server.enableCORS=false"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPlease upload your 'adult.csv' file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81a92034-af24-40a9-b9ec-c38c06521c4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81a92034-af24-40a9-b9ec-c38c06521c4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving adult 3.csv to adult 3.csv\n",
            "\n",
            "Uploaded file: adult 3.csv\n",
            "Dataset loaded successfully!\n",
            "First 5 rows of the dataset:\n",
            "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
            "0   25    Private  226802          11th                7       Never-married   \n",
            "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
            "4   18          ?  103497  Some-college               10       Never-married   \n",
            "\n",
            "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                  ?    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country income  \n",
            "0              40  United-States  <=50K  \n",
            "1              50  United-States  <=50K  \n",
            "2              40  United-States   >50K  \n",
            "3              40  United-States   >50K  \n",
            "4              30  United-States  <=50K  \n",
            "\n",
            "--- Data Cleaning Started ---\n",
            "Replaced '?' with 'Others' in categorical columns.\n",
            "Removed 'Without-pay' and 'Never-worked' from 'workclass'.\n",
            "Dropped the redundant 'education' column.\n",
            "Removed outliers. Rows changed from 48811 to 46720.\n",
            "\n",
            "Encoding categorical features...\n",
            "Encoded 'workclass'.\n",
            "Encoded 'marital-status'.\n",
            "Encoded 'occupation'.\n",
            "Encoded 'relationship'.\n",
            "Encoded 'race'.\n",
            "Encoded 'gender'.\n",
            "Encoded 'native-country'.\n",
            "\n",
            "Saved all label encoders to 'encoders.pkl'.\n",
            "Saved model column order to 'model_columns.pkl'.\n",
            "\n",
            "--- Data Cleaning and Preprocessing Complete! ---\n",
            "Final data preview (first 5 rows):\n",
            "   age  workclass  fnlwgt  educational-num  marital-status  occupation  \\\n",
            "0   25          3  226802                7               4           6   \n",
            "1   38          3   89814                9               2           4   \n",
            "2   28          1  336951               12               2          11   \n",
            "3   44          3  160323               10               2           6   \n",
            "4   18          2  103497               10               4           8   \n",
            "\n",
            "   relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
            "0             3     2       1             0             0              40   \n",
            "1             0     4       1             0             0              50   \n",
            "2             0     4       1             0             0              40   \n",
            "3             0     2       1          7688             0              40   \n",
            "4             3     4       0             0             0              30   \n",
            "\n",
            "   native-country income  \n",
            "0              39  <=50K  \n",
            "1              39  <=50K  \n",
            "2              39   >50K  \n",
            "3              39   >50K  \n",
            "4              39  <=50K  \n",
            "\n",
            "--- Model Training and Evaluation Started ---\n",
            "Data split into training (37376 rows) and testing (9344 rows) sets.\n",
            "\n",
            "Model: LogisticRegression\n",
            "Accuracy: 0.8261\n",
            "\n",
            "Model: RandomForest\n",
            "Accuracy: 0.8562\n",
            "\n",
            "Model: KNN\n",
            "Accuracy: 0.8276\n",
            "\n",
            "Model: SVM\n",
            "Accuracy: 0.8481\n",
            "\n",
            "Model: GradientBoosting\n",
            "Accuracy: 0.8645\n",
            "\n",
            "üèÜ Best performing model is GradientBoosting with an accuracy of 0.8645\n",
            "‚úÖ Saved the best model pipeline to 'best_model_pipeline.pkl'.\n",
            "\n",
            "--- Model Training and Evaluation Complete! ---\n",
            "‚úÖ Streamlit app code written to app.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-17T15:05:43+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-17T15:05:43+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-17T15:05:43+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-07-17T15:05:43+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No active tunnels to disconnect or an error occurred: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n",
            "Killed any lingering ngrok processes.\n",
            "üöÄ Your Streamlit app is live at: NgrokTunnel: \"https://ac56d938ce80.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "2025-07-17 15:05:44.641 \n",
            "Warning: the config option 'server.enableCORS=false' is not compatible with\n",
            "'server.enableXsrfProtection=true'.\n",
            "As a result, 'server.enableCORS' is being overridden to 'true'.\n",
            "\n",
            "More information:\n",
            "In order to protect against CSRF attacks, we send a cookie with each request.\n",
            "To do so, we must specify allowable origins, which places a restriction on\n",
            "cross-origin resource sharing.\n",
            "\n",
            "If cross origin resource sharing is required, please disable server.enableXsrfProtection.\n",
            "            \n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.62.161:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RfvS3P_E5U9R",
        "outputId": "75abb420-c7d0-47bb-baf3-0c9343f7b358"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}